{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n <center><h1 style=\"color:red\">Don't forget to upvote if you like it! :)</h1></center>","metadata":{}},{"cell_type":"markdown","source":"# About Iris dataset\n![](https://www.oreilly.com/library/view/python-artificial-intelligence/9781789539462/assets/462dc4fa-fd62-4539-8599-ac80a441382c.png)\nThe iris dataset contains the following data\n* 50 samples of 3 different species of iris (150 samples total)\n* Measurements: sepal length, sepal width, petal length, petal width\n* The format for the data: (sepal length, sepal width, petal length, petal width)","metadata":{}},{"cell_type":"markdown","source":"### The variables are:\n![](https://i.imgur.com/PQqYGaW.png)\n* sepal_length: Sepal length, in centimeters, used as input.\n* sepal_width: Sepal width, in centimeters, used as input.\n* petal_length: Petal length, in centimeters, used as input.\n* petal_width: Petal width, in centimeters, used as input.\n* class: Iris Setosa, Versicolor, or Virginica, used as the target.","metadata":{}},{"cell_type":"markdown","source":"# Contents\n### Data Preprocessing\n* Include Libraries\n* Import DataSet\n* Handle Missing Value\n\n### Data Visualization\n* Scatterplot\n* Pairplot\n* Barplot\n* Violin\n* Areaplot\n* Correlation\n\nFeature Engineering\n\n### Machine learning Model\n* Logistic Regression\n* Random Forest Classifier\n* Naive Bayes\n* KNN\t\n* Decision Tree\t\n* Support Vector Machine\n","metadata":{}},{"cell_type":"markdown","source":"# 1. Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### Importing pandas,numpy,matplotlib and Seaborn module ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importing Iris data set","metadata":{}},{"cell_type":"code","source":"iris=pd.read_csv('/kaggle/input/iris/Iris.csv')","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Displaying data","metadata":{}},{"cell_type":"code","source":"iris.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"iris['Species'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This data set has three varities of Iris plant.","metadata":{}},{"cell_type":"code","source":"iris.describe(include='all')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"iris.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see above data distribution of data points in each class is equal so Iris is a balanced dataset as the number of data points for every class is 50.","metadata":{}},{"cell_type":"markdown","source":"## Removing the unneeded column","metadata":{}},{"cell_type":"code","source":"iris.drop(columns=\"Id\",inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking if there are any missing values\n![](https://blogs.worldbank.org/sites/default/files/opendata/missing-data.jpg)","metadata":{}},{"cell_type":"code","source":"iris.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import missingno as msno\nmsno.bar(iris,figsize=(8,6),color='skyblue')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is not any missing  value in this dataset","metadata":{}},{"cell_type":"markdown","source":"# 2. Data Visualization","metadata":{}},{"cell_type":"markdown","source":"## 2.1 Scatterplot\nA scatter plot is a two-dimensional data visualization that uses dots to represent the values obtained for two different variables — one plotted along the x-axis and the other plotted along the y-axis\nwe can plot the scatter plot between any two features.\ni’am taking an example of petal length and petal width.","metadata":{}},{"cell_type":"code","source":"g=sns.relplot(x='SepalLengthCm',y='SepalWidthCm',data=iris,hue='Species',style='Species')\ng.fig.set_size_inches(10,5)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g=sns.relplot(x='PetalLengthCm',y='PetalWidthCm',data=iris,hue='Species',style='Species')\ng.fig.set_size_inches(10,5)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see that the Petal Features are giving a better cluster division compared to the Sepal features. This is an indication that the Petals can help in better and accurate Predictions over the Sepal.","metadata":{}},{"cell_type":"markdown","source":"## 2.2 Pairplot\nPair Plots are a really simple (one-line-of-code simple!) way to visualize relationships between each variable. It produces a matrix of relationships between each variable in your data for an instant examination of our data.\npair plot gives scatter plot of different features.\npair plot for iris data set.","metadata":{}},{"cell_type":"code","source":"sns.pairplot(iris,hue=\"Species\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from the graph we can see the scatter plot between the any two features and the distributions.\nfrom the distributions above peatl length is separating the iris setosa from remaining .\nfrom plot between petal length and petal width we can separate the flowers","metadata":{}},{"cell_type":"markdown","source":"## 2.3 BoxPlot \n boxplot is a standardized way of displaying the distribution of data based on a five number summary (“minimum”, first quartile (Q1), median, third quartile (Q3), and “maximum”). It can tell you about your outliers and what their values are. It can also tell you if your data is symmetrical, how tightly your data is grouped, and if and how your data is skewed.\n![](https://miro.medium.com/proxy/1*2c21SkzJMf3frPXPAR_gZA.png) ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.subplot(2,2,1)\nsns.boxplot(x='Species',y='PetalLengthCm',data=iris)\nplt.subplot(2,2,2)\nsns.boxplot(x='Species',y='PetalWidthCm',data=iris)\nplt.subplot(2,2,3)\nsns.boxplot(x='Species',y='SepalLengthCm',data=iris)\nplt.subplot(2,2,4)\nsns.boxplot(x='Species',y='SepalWidthCm',data=iris)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplots(figsize=(10,7))\nsns.boxplot(data=iris).set_title(\"Distribution of Sepal_length, Sepal_width, petal_length and petal_width of 3 flowers\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.4 Violin\nViolin Plot is a method to visualize the distribution of numerical data of different variables. It is similar to Box Plot but with a rotated plot on each side, giving more information about the density estimate on the y-axis.\nThe density is mirrored and flipped over and the resulting shape is filled in, creating an image resembling a violin. The advantage of a violin plot is that it can show nuances in the distribution that aren’t perceptible in a boxplot. On the other hand, the boxplot more clearly shows the outliers in the data.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.subplot(2,2,1)\nsns.violinplot(x='Species',y='PetalLengthCm',data=iris)\nplt.subplot(2,2,2)\nsns.violinplot(x='Species',y='PetalWidthCm',data=iris)\nplt.subplot(2,2,3)\nsns.violinplot(x='Species',y='SepalLengthCm',data=iris)\nplt.subplot(2,2,4)\nsns.violinplot(x='Species',y='SepalWidthCm',data=iris)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplots(figsize=(10,7))\nsns.violinplot(data=iris)\nsns.swarmplot( data=iris)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.5 Area Plot \nArea Plot gives us a visual representation of Various dimensions of Iris flower and their range in dataset.","metadata":{}},{"cell_type":"code","source":"iris.plot.area(y=['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm'],alpha=0.4,figsize=(12, 6));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.6 Correlation\n![](https://www.mathsisfun.com/data/images/correlation-examples.svg)","metadata":{}},{"cell_type":"markdown","source":"Now, when we train any algorithm, the number of features and their correlation plays an important role. If there are features and many of the features are highly correlated, then training an algorithm with all the featues will reduce the accuracy. Thus features selection should be done carefully. This dataset has less featues but still we will see the correlation.","metadata":{}},{"cell_type":"code","source":"iris.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplots(figsize = (8,8))\nsns.heatmap(iris.corr(),annot=True,fmt=\"f\").set_title(\"Corelation of attributes (petal length,width and sepal length,width) among Iris species\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation--->\n\nThe Sepal Width and Length are not correlated The Petal Width and Length are highly correlated\n\nWe will use all the features for training the algorithm and check the accuracy.\n","metadata":{}},{"cell_type":"markdown","source":"## Dividing data into features and labels\n![](https://miro.medium.com/max/1002/1*68H8EsCwfqJNxzYdPYtEDw.png)\nAs we can see dataset contain six columns: Id, SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm and Species. The actual features are described by columns 1-4. Last column contains labels of samples. Firstly we need to split data into two arrays: X (features) and y (labels).","metadata":{}},{"cell_type":"code","source":"X=iris.iloc[:,0:4].values\ny=iris.iloc[:,4].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Label encoding\n![](https://miro.medium.com/max/772/1*QQe-4476Oy3_dI1vhb3dDg.png)\nAs we can see labels are categorical. KNeighborsClassifier does not accept string labels. We need to use LabelEncoder to transform them into numbers. Iris-setosa correspond to 0, Iris-versicolor correspond to 1 and Iris-virginica correspond to 2.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Building Machine Learning Models","metadata":{}},{"cell_type":"code","source":"\n#Metrics\nfrom sklearn.metrics import make_scorer, accuracy_score,precision_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score ,precision_score,recall_score,f1_score\n\n#Model Select\nfrom sklearn.model_selection import KFold,train_test_split,cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import  LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import linear_model\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting The Data into Training And Testing Dataset\n![](https://data-flair.training/blogs/wp-content/uploads/sites/2/2018/08/1-16.png)","metadata":{}},{"cell_type":"code","source":"#Train and Test split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will train several Machine Learning models and compare their results. Note that because the dataset does not provide labels for their testing-set, we need to use the predictions on the training set to compare the algorithms with each other.","metadata":{}},{"cell_type":"markdown","source":"### 3.1 Random Forest:\nRandom forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction.\n\nA large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.\n![](https://miro.medium.com/max/1170/1*58f1CZ8M4il0OZYg2oRN4w.png)","metadata":{}},{"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\nY_prediction = random_forest.predict(X_test)\naccuracy_rf=round(accuracy_score(y_test,Y_prediction)* 100, 2)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\n\n\ncm = confusion_matrix(y_test, Y_prediction)\naccuracy = accuracy_score(y_test,Y_prediction)\nprecision =precision_score(y_test, Y_prediction,average='micro')\nrecall =  recall_score(y_test, Y_prediction,average='micro')\nf1 = f1_score(y_test,Y_prediction,average='micro')\nprint('Confusion matrix for Random Forest\\n',cm)\nprint('accuracy_random_Forest : %.3f' %accuracy)\nprint('precision_random_Forest : %.3f' %precision)\nprint('recall_random_Forest : %.3f' %recall)\nprint('f1-score_random_Forest : %.3f' %f1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2 Logistic Regression:\nLogistic Regression is a Machine Learning algorithm which is used for the classification problems, it is a predictive analysis algorithm and based on the concept of probability.\n\nWe can call a Logistic Regression a Linear Regression model but the Logistic Regression uses a more complex cost function, this cost function can be defined as the ‘Sigmoid function’ or also known as the ‘logistic function’ instead of a linear function.\n![](https://miro.medium.com/max/570/1*50TdLe6f_AW8wnBBkyLYgw.png)","metadata":{}},{"cell_type":"code","source":"logreg = LogisticRegression(solver= 'lbfgs',max_iter=400)\nlogreg.fit(X_train, y_train)\nY_pred = logreg.predict(X_test)\naccuracy_lr=round(accuracy_score(y_test,Y_pred)* 100, 2)\nacc_log = round(logreg.score(X_train, y_train) * 100, 2)\n\n\ncm = confusion_matrix(y_test, Y_pred,)\naccuracy = accuracy_score(y_test,Y_pred)\nprecision =precision_score(y_test, Y_pred,average='micro')\nrecall =  recall_score(y_test, Y_pred,average='micro')\nf1 = f1_score(y_test,Y_pred,average='micro')\nprint('Confusion matrix for Logistic Regression\\n',cm)\nprint('accuracy_Logistic Regression : %.3f' %accuracy)\nprint('precision_Logistic Regression : %.3f' %precision)\nprint('recall_Logistic Regression: %.3f' %recall)\nprint('f1-score_Logistic Regression : %.3f' %f1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3 K Nearest Neighbor:\nK-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique.\n\nK-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories.\n\nK-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm.\n![](https://www.kdnuggets.com/wp-content/uploads/rapidminer-knn-image1.jpg)","metadata":{}},{"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\nY_pred = knn.predict(X_test) \naccuracy_knn=round(accuracy_score(y_test,Y_pred)* 100, 2)\nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\n\ncm = confusion_matrix(y_test, Y_pred)\naccuracy = accuracy_score(y_test,Y_pred)\nprecision =precision_score(y_test, Y_pred,average='micro')\nrecall =  recall_score(y_test, Y_pred,average='micro')\nf1 = f1_score(y_test,Y_pred,average='micro')\nprint('Confusion matrix for KNN\\n',cm)\nprint('accuracy_KNN : %.3f' %accuracy)\nprint('precision_KNN : %.3f' %precision)\nprint('recall_KNN: %.3f' %recall)\nprint('f1-score_KNN : %.3f' %f1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's check the accuracy for various values of n for K-Nearest nerighbours","metadata":{}},{"cell_type":"code","source":"plt.subplots(figsize=(20,5))\na_index=list(range(1,50))\na=pd.Series()\nx=range(1,50)\n#x=[1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,50)):\n    model=KNeighborsClassifier(n_neighbors=i) \n    model.fit(X_train, y_train) \n    prediction=model.predict(X_test)\n    a=a.append(pd.Series(accuracy_score(y_test,prediction)))\nplt.plot(a_index, a,marker=\"*\")\nplt.xticks(x)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above is the graph showing the accuracy for the KNN models using different values of n.","metadata":{}},{"cell_type":"markdown","source":"### 3.4 Gaussian Naive Bayes:\nNaive Bayes is a classification algorithm for binary (two-class) and multi-class classification problems. The technique is easiest to understand when described using binary or categorical input values.\n\nIt is called naive Bayes or idiot Bayes because the calculation of the probabilities for each hypothesis are simplified to make their calculation tractable. Rather than attempting to calculate the values of each attribute value P(d1, d2, d3|h), they are assumed to be conditionally independent given the target value and calculated as P(d1|h) * P(d2|H) and so on.\n\nThis is a very strong assumption that is most unlikely in real data, i.e. that the attributes do not interact. Nevertheless, the approach performs surprisingly well on data where this assumption does not hold.\n![](https://miro.medium.com/max/1200/0*qFuHAV7Vd09064q-.jpeg)","metadata":{}},{"cell_type":"code","source":"gaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\nY_pred = gaussian.predict(X_test) \naccuracy_nb=round(accuracy_score(y_test,Y_pred)* 100, 2)\nacc_gaussian = round(gaussian.score(X_train, y_train) * 100, 2)\n\ncm = confusion_matrix(y_test, Y_pred)\naccuracy = accuracy_score(y_test,Y_pred)\nprecision =precision_score(y_test, Y_pred,average='micro')\nrecall =  recall_score(y_test, Y_pred,average='micro')\nf1 = f1_score(y_test,Y_pred,average='micro')\nprint('Confusion matrix for Naive Bayes\\n',cm)\nprint('accuracy_Naive Bayes: %.3f' %accuracy)\nprint('precision_Naive Bayes: %.3f' %precision)\nprint('recall_Naive Bayes: %.3f' %recall)\nprint('f1-score_Naive Bayes : %.3f' %f1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.5 Linear Support Vector Machine:\nSupport Vector Machine” (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well\n![](https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2017/02/Margin.png)","metadata":{}},{"cell_type":"code","source":"linear_svc = LinearSVC(max_iter=4000)\nlinear_svc.fit(X_train, y_train)\nY_pred = linear_svc.predict(X_test)\naccuracy_svc=round(accuracy_score(y_test,Y_pred)* 100, 2)\nacc_linear_svc = round(linear_svc.score(X_train, y_train) * 100, 2)\n\ncm = confusion_matrix(y_test, Y_pred)\naccuracy = accuracy_score(y_test,Y_pred)\nprecision =precision_score(y_test, Y_pred,average='micro')\nrecall =  recall_score(y_test, Y_pred,average='micro')\nf1 = f1_score(y_test,Y_pred,average='micro')\nprint('Confusion matrix for SVC\\n',cm)\nprint('accuracy_SVC: %.3f' %accuracy)\nprint('precision_SVC: %.3f' %precision)\nprint('recall_SVC: %.3f' %recall)\nprint('f1-score_SVC : %.3f' %f1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.6 Decision Tree:\nA decision tree is a flowchart-like structure in which each internal node represents a test on a feature (e.g. whether a coin flip comes up heads or tails) , each leaf node represents a class label (decision taken after computing all features) and branches represent conjunctions of features that lead to those class labels. The paths from root to leaf represent classification rules.\n![](https://miro.medium.com/max/1000/1*LMoJmXCsQlciGTEyoSN39g.jpeg)","metadata":{}},{"cell_type":"code","source":"decision_tree = DecisionTreeClassifier() \ndecision_tree.fit(X_train, y_train)  \nY_pred = decision_tree.predict(X_test) \naccuracy_dt=round(accuracy_score(y_test,Y_pred)* 100, 2)\nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\n\ncm = confusion_matrix(y_test, Y_pred)\naccuracy = accuracy_score(y_test,Y_pred)\nprecision =precision_score(y_test, Y_pred,average='micro')\nrecall =  recall_score(y_test, Y_pred,average='micro')\nf1 = f1_score(y_test,Y_pred,average='micro')\nprint('Confusion matrix for DecisionTree\\n',cm)\nprint('accuracy_DecisionTree: %.3f' %accuracy)\nprint('precision_DecisionTree: %.3f' %precision)\nprint('recall_DecisionTree: %.3f' %recall)\nprint('f1-score_DecisionTree : %.3f' %f1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import plot_tree\nplt.figure(figsize = (15,10))\nplot_tree(decision_tree.fit(X_train, y_train)  ,filled=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Which is the best Model ?","metadata":{}},{"cell_type":"code","source":"results = pd.DataFrame({\n    'Model': [ 'KNN', \n              'Logistic Regression', \n              'Random Forest',\n              'Naive Bayes',  \n              ' Support Vector Machine', \n              'Decision Tree'],\n    'Score': [ acc_knn,\n              acc_log, \n              acc_random_forest,\n              acc_gaussian,  \n              acc_linear_svc,\n              acc_decision_tree],\n    \"Accuracy_score\":[accuracy_knn,\n                      accuracy_lr,\n                      accuracy_rf,\n                      accuracy_nb,\n                      accuracy_svc,\n                      accuracy_dt\n                     ]})\nresult_df = results.sort_values(by='Accuracy_score', ascending=False)\nresult_df = result_df.reset_index(drop=True)\nresult_df.head(9)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see best Model is given by Naive Bayes(100% Accuracy).","metadata":{}},{"cell_type":"code","source":"plt.subplots(figsize=(12,8))\nax=sns.barplot(x='Model',y=\"Accuracy_score\",data=result_df)\nlabels = (result_df[\"Accuracy_score\"])\n# add result numbers on barchart\nfor i, v in enumerate(labels):\n    ax.text(i, v+1, str(v), horizontalalignment = 'center', size = 15, color = 'black')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations:\nThis was expected as we saw in the heatmap above that the correlation between the Sepal Width and Length was very low whereas the correlation between Petal Width and Length was very high.\nThus we have just implemented some of the common Machine Learning. Since the dataset is small with very few features.","metadata":{}},{"cell_type":"markdown","source":"If you have reached till here, So i hope you liked my Analysis.\n\nDon't forget to **upvote** if you like it!.\n\nI'm a beginner and any suggestion in the comment box is highly appreciated.\n\nIf you have any doubt reagrding any part of the notebook, feel free to comment your doubt in the comment box.\n\nThank you!!","metadata":{}}]}